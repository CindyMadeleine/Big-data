{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "This notebook contains examples and code explainig how to use linear regression for supervised learning.\n",
    "The code is based on several examples from the internet - credits at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to least square method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset is: [Price, Demand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset = (x1, y1 ), (x2, y2 ), ..., (x5, y5)\n",
    "# y is dependent and x is the independent variables.\n",
    "input = np.array([[37, 46],\n",
    "                  [57, 26],\n",
    "                  [77, 67],\n",
    "                  [87, 14],\n",
    "                  [107, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = number of observations. \n",
    "# Amount of Y's.\n",
    "n = np.shape(input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix\n",
    "# To transpose a matrix you write an .T at the end of the matrix\n",
    "# X = (1, x1), (1, x2), ..., (1, x5)\n",
    "X = np.matrix([np.ones(n), input[:,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  37.]\n",
      " [  1.  57.]\n",
      " [  1.  77.]\n",
      " [  1.  87.]\n",
      " [  1. 107.]]\n"
     ]
    }
   ],
   "source": [
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input all the values from coloumn two\n",
    "# Transpose it by writing a T at the end\n",
    "# Y = y1, y2, ..., y5\n",
    "Y = np.matrix(input[:,1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46]\n",
      " [26]\n",
      " [67]\n",
      " [14]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot is an operation in order to multiply two matrices. You call that matrix on the actuall matrix.\n",
    "# .T transpose the matrix\n",
    "# inalg.inv invert the matrix\n",
    "\n",
    "# A = (X^T * X)^-1 * X^T * Y\n",
    "A = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64.3       ]\n",
      " [-0.43424658]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value of A at position (0,0).\n",
    "# b is the constant or intercept\n",
    "b = A[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.29999999999995\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value of A at position (1,0).\n",
    "# m is the coefficient\n",
    "m = A[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4342465753424659\n"
     ]
    }
   ],
   "source": [
    "print (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.  25.  50.  75. 100. 125.]\n"
     ]
    }
   ],
   "source": [
    "# Make an array of some numbers between 0 and 125 in steps of 6\n",
    "# xx gives  the range of the X value\n",
    "xx = np.linspace(0.0, 125, num=6, retstep=False)\n",
    "print(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64.3        53.44383562 42.58767123 31.73150685 20.87534247 10.01917808]]\n"
     ]
    }
   ],
   "source": [
    "# Every element will be multiplied by A[1] and added by A[1]\n",
    "# array creates a new array\n",
    "# yy gives the range of the Y values\n",
    "yy = np.array(A[0] + A[1] * xx)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data, regression line\n",
    "plt.figure(1)\n",
    "\n",
    "\n",
    "# drawing the blue line in the coordinate system. \n",
    "# Assign the xx to X\n",
    "# Assign the yy to Y\n",
    "# then assign a blue color\n",
    "plt.plot(xx, yy.T, color='b')\n",
    "\n",
    "# Add the dots in the coordinate system. \n",
    "# Assign input at coloumn 0 to x\n",
    "# assign input at coloumn 1 to y\n",
    "# then assign a red color.\n",
    "plt.scatter(input[:,0], input[:,1], color='r')\n",
    "\n",
    "# if the dots are close to the line, the model is good. The error is distancly to each of the dots and the line\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 37  57  77  87 107]\n",
      "[46 26 67 14 10]\n"
     ]
    }
   ],
   "source": [
    "print(input[:,0])\n",
    "print(input[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
      "        4.9800e+00],\n",
      "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
      "        9.1400e+00],\n",
      "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
      "        4.0300e+00],\n",
      "       ...,\n",
      "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
      "        5.6400e+00],\n",
      "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
      "        6.4800e+00],\n",
      "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
      "        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
      "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
      "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
      "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
      "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
      "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
      "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
      "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
      "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
      "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
      "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
      "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
      "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
      "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
      "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
      "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
      "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
      "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
      "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
      "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
      "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
      "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
      "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
      "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
      "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
      "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
      "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
      "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
      "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
      "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
      "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
      "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
      "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
      "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
      "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
      "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
      "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
      "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
      "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
      "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
      "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
      "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
      "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
      "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
      "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
      "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
      "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'DESCR': \"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\"}\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "print(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 75. ]\n",
      " [ 75. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 75. ]\n",
      " [ 90. ]\n",
      " [ 85. ]\n",
      " [100. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 17.5]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 28. ]\n",
      " [ 28. ]\n",
      " [ 28. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 60. ]\n",
      " [ 60. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 95. ]\n",
      " [ 95. ]\n",
      " [ 82.5]\n",
      " [ 82.5]\n",
      " [ 95. ]\n",
      " [ 95. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 90. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 90. ]\n",
      " [ 90. ]\n",
      " [ 55. ]\n",
      " [ 80. ]\n",
      " [ 52.5]\n",
      " [ 52.5]\n",
      " [ 52.5]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 70. ]\n",
      " [ 70. ]\n",
      " [ 70. ]\n",
      " [ 34. ]\n",
      " [ 34. ]\n",
      " [ 34. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 35. ]\n",
      " [ 35. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 35. ]\n",
      " [  0. ]\n",
      " [ 55. ]\n",
      " [ 55. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 85. ]\n",
      " [ 80. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 60. ]\n",
      " [ 60. ]\n",
      " [ 90. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Use only one feature\n",
    "# np.newaxis increase the dimension of existing array by one dimension, 2D becomes 3D.\n",
    "boston_X = boston.data[:, np.newaxis, 1]\n",
    "print(boston_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10\n",
    "\n",
    "# prepare the training data\n",
    "boston_X_train = boston_X[:step]\n",
    "\n",
    "# prepare the test data\n",
    "boston_X_test = boston_X[step:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: \n",
      " [[18. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [12.5]\n",
      " [12.5]\n",
      " [12.5]\n",
      " [12.5]]\n",
      "train test: \n",
      " [[ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 75. ]\n",
      " [ 75. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 21. ]\n",
      " [ 75. ]\n",
      " [ 90. ]\n",
      " [ 85. ]\n",
      " [100. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 17.5]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [ 12.5]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [ 25. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 28. ]\n",
      " [ 28. ]\n",
      " [ 28. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 45. ]\n",
      " [ 60. ]\n",
      " [ 60. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 95. ]\n",
      " [ 95. ]\n",
      " [ 82.5]\n",
      " [ 82.5]\n",
      " [ 95. ]\n",
      " [ 95. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 30. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 90. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 20. ]\n",
      " [ 90. ]\n",
      " [ 90. ]\n",
      " [ 55. ]\n",
      " [ 80. ]\n",
      " [ 52.5]\n",
      " [ 52.5]\n",
      " [ 52.5]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 70. ]\n",
      " [ 70. ]\n",
      " [ 70. ]\n",
      " [ 34. ]\n",
      " [ 34. ]\n",
      " [ 34. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [ 33. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 35. ]\n",
      " [ 35. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 35. ]\n",
      " [  0. ]\n",
      " [ 55. ]\n",
      " [ 55. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [ 85. ]\n",
      " [ 80. ]\n",
      " [ 40. ]\n",
      " [ 40. ]\n",
      " [ 60. ]\n",
      " [ 60. ]\n",
      " [ 90. ]\n",
      " [ 80. ]\n",
      " [ 80. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]\n",
      " [  0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"train set: \\n\", boston_X_train)\n",
    "\n",
    "\n",
    "print(\"train test: \\n\", boston_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: The progression of the matrix\n",
    "# prepare the training data of target\n",
    "boston_y_train = boston.target[:step]\n",
    "\n",
    "# prepare the test data of target\n",
    "boston_y_test = boston.target[step:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  [24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9]\n",
      "test:  [15.  18.9 21.7 20.4 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5\n",
      " 15.6 13.9 16.6 14.8 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.\n",
      " 24.7 30.8 34.9 26.6 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5\n",
      " 25.  23.4 18.9 35.4 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5\n",
      " 19.4 22.  17.4 20.9 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3\n",
      " 28.  23.9 24.8 22.9 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.\n",
      " 20.6 28.4 21.4 38.7 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4\n",
      " 19.8 19.4 21.7 22.8 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3\n",
      " 20.5 17.3 18.8 21.4 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1\n",
      " 17.4 17.1 13.3 17.8 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4\n",
      " 21.5 19.6 15.3 19.4 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.\n",
      " 22.7 25.  50.  23.8 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6\n",
      " 29.9 37.2 39.8 36.2 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5\n",
      " 36.4 31.1 29.1 50.  33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6\n",
      " 24.4 22.5 24.4 20.  21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.\n",
      " 26.7 21.7 27.5 30.1 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3\n",
      " 29.  24.  25.1 31.5 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5\n",
      " 24.5 26.2 24.4 24.8 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1\n",
      " 48.8 31.  36.5 22.8 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.\n",
      " 33.2 33.1 29.1 35.1 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8\n",
      " 28.5 37.3 27.9 23.9 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1\n",
      " 36.1 28.4 33.4 28.2 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8\n",
      " 23.1 21.  23.8 23.1 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1\n",
      " 19.4 22.2 20.7 21.1 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5\n",
      " 17.2 23.1 24.5 26.6 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6\n",
      " 25.  19.9 20.8 16.8 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8\n",
      " 13.8 15.  13.9 13.3 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4\n",
      " 10.2 11.5 15.1 23.2  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2\n",
      " 12.1  8.3  8.5  5.  11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2\n",
      "  7.5 10.4  8.8  8.4 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5\n",
      " 14.5 14.1 16.1 14.3 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4\n",
      " 10.8 11.8 14.9 12.6 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5\n",
      " 14.9 20.  16.4 17.7 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6\n",
      " 23.2 29.8 13.8 13.3 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2\n",
      " 19.1 20.6 15.2  7.   8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5\n",
      " 16.8 22.4 20.6 23.9 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(\"target: \", boston_y_train)\n",
    "\n",
    "print(\"test: \", boston_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(boston_X_train, boston_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.60768598]\n",
      "Mean squared error: 387.05\n",
      "Variance score: -3.55\n"
     ]
    }
   ],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "# The larger the error, the worse the model. \n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((regr.predict(boston_X_test) - boston_y_test) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "# The less the explained variance score the less the model will explain.\n",
    "print('Variance score: %.2f' % regr.score(boston_X_test, boston_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot outputs\n",
    "# scatter plots the blue line\n",
    "# blue line is our model of \n",
    "plt.scatter(boston_X_test, boston_y_test,  color='black')\n",
    "\n",
    "# plot plots the dots\n",
    "plt.plot(boston_X_test, regr.predict(boston_X_test), color='blue',\n",
    "         linewidth=3)\n",
    "\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "\n",
    "# the dots are spread due to the low explained variance score\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.core import datetools\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets ## imports datasets from scikit-learn\n",
    "winedata = datasets.load_wine() ## loads Wine dataset from datasets library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Data Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- 1) Alcohol\n",
      " \t\t- 2) Malic acid\n",
      " \t\t- 3) Ash\n",
      "\t\t- 4) Alcalinity of ash  \n",
      " \t\t- 5) Magnesium\n",
      "\t\t- 6) Total phenols\n",
      " \t\t- 7) Flavanoids\n",
      " \t\t- 8) Nonflavanoid phenols\n",
      " \t\t- 9) Proanthocyanins\n",
      "\t\t- 10)Color intensity\n",
      " \t\t- 11)Hue\n",
      " \t\t- 12)OD280/OD315 of diluted wines\n",
      " \t\t- 13)Proline\n",
      "        \t- class:\n",
      "                - class_0\n",
      "                - class_1\n",
      "                - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      "References\n",
      "----------\n",
      "(1) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "Comparison of Classifiers in High Dimensional Settings, \n",
      "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Technometrics). \n",
      "\n",
      "The data was used with many others for comparing various \n",
      "classifiers. The classes are separable, though only RDA \n",
      "has achieved 100% correct classification. \n",
      "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "(All results using the leave-one-out technique) \n",
      "\n",
      "(2) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Journal of Chemometrics). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(winedata.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The independent variable\n",
    "winedata.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dependent variable\n",
    "winedata.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data to fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "5      14.20        1.76  2.45               15.2      112.0           3.27   \n",
      "6      14.39        1.87  2.45               14.6       96.0           2.50   \n",
      "7      14.06        2.15  2.61               17.6      121.0           2.60   \n",
      "8      14.83        1.64  2.17               14.0       97.0           2.80   \n",
      "9      13.86        1.35  2.27               16.0       98.0           2.98   \n",
      "10     14.10        2.16  2.30               18.0      105.0           2.95   \n",
      "11     14.12        1.48  2.32               16.8       95.0           2.20   \n",
      "12     13.75        1.73  2.41               16.0       89.0           2.60   \n",
      "13     14.75        1.73  2.39               11.4       91.0           3.10   \n",
      "14     14.38        1.87  2.38               12.0      102.0           3.30   \n",
      "15     13.63        1.81  2.70               17.2      112.0           2.85   \n",
      "16     14.30        1.92  2.72               20.0      120.0           2.80   \n",
      "17     13.83        1.57  2.62               20.0      115.0           2.95   \n",
      "18     14.19        1.59  2.48               16.5      108.0           3.30   \n",
      "19     13.64        3.10  2.56               15.2      116.0           2.70   \n",
      "20     14.06        1.63  2.28               16.0      126.0           3.00   \n",
      "21     12.93        3.80  2.65               18.6      102.0           2.41   \n",
      "22     13.71        1.86  2.36               16.6      101.0           2.61   \n",
      "23     12.85        1.60  2.52               17.8       95.0           2.48   \n",
      "24     13.50        1.81  2.61               20.0       96.0           2.53   \n",
      "25     13.05        2.05  3.22               25.0      124.0           2.63   \n",
      "26     13.39        1.77  2.62               16.1       93.0           2.85   \n",
      "27     13.30        1.72  2.14               17.0       94.0           2.40   \n",
      "28     13.87        1.90  2.80               19.4      107.0           2.95   \n",
      "29     14.02        1.68  2.21               16.0       96.0           2.65   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "148    13.32        3.24  2.38               21.5       92.0           1.93   \n",
      "149    13.08        3.90  2.36               21.5      113.0           1.41   \n",
      "150    13.50        3.12  2.62               24.0      123.0           1.40   \n",
      "151    12.79        2.67  2.48               22.0      112.0           1.48   \n",
      "152    13.11        1.90  2.75               25.5      116.0           2.20   \n",
      "153    13.23        3.30  2.28               18.5       98.0           1.80   \n",
      "154    12.58        1.29  2.10               20.0      103.0           1.48   \n",
      "155    13.17        5.19  2.32               22.0       93.0           1.74   \n",
      "156    13.84        4.12  2.38               19.5       89.0           1.80   \n",
      "157    12.45        3.03  2.64               27.0       97.0           1.90   \n",
      "158    14.34        1.68  2.70               25.0       98.0           2.80   \n",
      "159    13.48        1.67  2.64               22.5       89.0           2.60   \n",
      "160    12.36        3.83  2.38               21.0       88.0           2.30   \n",
      "161    13.69        3.26  2.54               20.0      107.0           1.83   \n",
      "162    12.85        3.27  2.58               22.0      106.0           1.65   \n",
      "163    12.96        3.45  2.35               18.5      106.0           1.39   \n",
      "164    13.78        2.76  2.30               22.0       90.0           1.35   \n",
      "165    13.73        4.36  2.26               22.5       88.0           1.28   \n",
      "166    13.45        3.70  2.60               23.0      111.0           1.70   \n",
      "167    12.82        3.37  2.30               19.5       88.0           1.48   \n",
      "168    13.58        2.58  2.69               24.5      105.0           1.55   \n",
      "169    13.40        4.60  2.86               25.0      112.0           1.98   \n",
      "170    12.20        3.03  2.32               19.0       96.0           1.25   \n",
      "171    12.77        2.39  2.28               19.5       86.0           1.39   \n",
      "172    14.16        2.51  2.48               20.0       91.0           1.68   \n",
      "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
      "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
      "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
      "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
      "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0          3.06                  0.28             2.29         5.640000  1.04   \n",
      "1          2.76                  0.26             1.28         4.380000  1.05   \n",
      "2          3.24                  0.30             2.81         5.680000  1.03   \n",
      "3          3.49                  0.24             2.18         7.800000  0.86   \n",
      "4          2.69                  0.39             1.82         4.320000  1.04   \n",
      "5          3.39                  0.34             1.97         6.750000  1.05   \n",
      "6          2.52                  0.30             1.98         5.250000  1.02   \n",
      "7          2.51                  0.31             1.25         5.050000  1.06   \n",
      "8          2.98                  0.29             1.98         5.200000  1.08   \n",
      "9          3.15                  0.22             1.85         7.220000  1.01   \n",
      "10         3.32                  0.22             2.38         5.750000  1.25   \n",
      "11         2.43                  0.26             1.57         5.000000  1.17   \n",
      "12         2.76                  0.29             1.81         5.600000  1.15   \n",
      "13         3.69                  0.43             2.81         5.400000  1.25   \n",
      "14         3.64                  0.29             2.96         7.500000  1.20   \n",
      "15         2.91                  0.30             1.46         7.300000  1.28   \n",
      "16         3.14                  0.33             1.97         6.200000  1.07   \n",
      "17         3.40                  0.40             1.72         6.600000  1.13   \n",
      "18         3.93                  0.32             1.86         8.700000  1.23   \n",
      "19         3.03                  0.17             1.66         5.100000  0.96   \n",
      "20         3.17                  0.24             2.10         5.650000  1.09   \n",
      "21         2.41                  0.25             1.98         4.500000  1.03   \n",
      "22         2.88                  0.27             1.69         3.800000  1.11   \n",
      "23         2.37                  0.26             1.46         3.930000  1.09   \n",
      "24         2.61                  0.28             1.66         3.520000  1.12   \n",
      "25         2.68                  0.47             1.92         3.580000  1.13   \n",
      "26         2.94                  0.34             1.45         4.800000  0.92   \n",
      "27         2.19                  0.27             1.35         3.950000  1.02   \n",
      "28         2.97                  0.37             1.76         4.500000  1.25   \n",
      "29         2.33                  0.26             1.98         4.700000  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "148        0.76                  0.45             1.25         8.420000  0.55   \n",
      "149        1.39                  0.34             1.14         9.400000  0.57   \n",
      "150        1.57                  0.22             1.25         8.600000  0.59   \n",
      "151        1.36                  0.24             1.26        10.800000  0.48   \n",
      "152        1.28                  0.26             1.56         7.100000  0.61   \n",
      "153        0.83                  0.61             1.87        10.520000  0.56   \n",
      "154        0.58                  0.53             1.40         7.600000  0.58   \n",
      "155        0.63                  0.61             1.55         7.900000  0.60   \n",
      "156        0.83                  0.48             1.56         9.010000  0.57   \n",
      "157        0.58                  0.63             1.14         7.500000  0.67   \n",
      "158        1.31                  0.53             2.70        13.000000  0.57   \n",
      "159        1.10                  0.52             2.29        11.750000  0.57   \n",
      "160        0.92                  0.50             1.04         7.650000  0.56   \n",
      "161        0.56                  0.50             0.80         5.880000  0.96   \n",
      "162        0.60                  0.60             0.96         5.580000  0.87   \n",
      "163        0.70                  0.40             0.94         5.280000  0.68   \n",
      "164        0.68                  0.41             1.03         9.580000  0.70   \n",
      "165        0.47                  0.52             1.15         6.620000  0.78   \n",
      "166        0.92                  0.43             1.46        10.680000  0.85   \n",
      "167        0.66                  0.40             0.97        10.260000  0.72   \n",
      "168        0.84                  0.39             1.54         8.660000  0.74   \n",
      "169        0.96                  0.27             1.11         8.500000  0.67   \n",
      "170        0.49                  0.40             0.73         5.500000  0.66   \n",
      "171        0.51                  0.48             0.64         9.899999  0.57   \n",
      "172        0.70                  0.44             1.24         9.700000  0.62   \n",
      "173        0.61                  0.52             1.06         7.700000  0.64   \n",
      "174        0.75                  0.43             1.41         7.300000  0.70   \n",
      "175        0.69                  0.43             1.35        10.200000  0.59   \n",
      "176        0.68                  0.53             1.46         9.300000  0.60   \n",
      "177        0.76                  0.56             1.35         9.200000  0.61   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  \n",
      "0                            3.92   1065.0  \n",
      "1                            3.40   1050.0  \n",
      "2                            3.17   1185.0  \n",
      "3                            3.45   1480.0  \n",
      "4                            2.93    735.0  \n",
      "5                            2.85   1450.0  \n",
      "6                            3.58   1290.0  \n",
      "7                            3.58   1295.0  \n",
      "8                            2.85   1045.0  \n",
      "9                            3.55   1045.0  \n",
      "10                           3.17   1510.0  \n",
      "11                           2.82   1280.0  \n",
      "12                           2.90   1320.0  \n",
      "13                           2.73   1150.0  \n",
      "14                           3.00   1547.0  \n",
      "15                           2.88   1310.0  \n",
      "16                           2.65   1280.0  \n",
      "17                           2.57   1130.0  \n",
      "18                           2.82   1680.0  \n",
      "19                           3.36    845.0  \n",
      "20                           3.71    780.0  \n",
      "21                           3.52    770.0  \n",
      "22                           4.00   1035.0  \n",
      "23                           3.63   1015.0  \n",
      "24                           3.82    845.0  \n",
      "25                           3.20    830.0  \n",
      "26                           3.22   1195.0  \n",
      "27                           2.77   1285.0  \n",
      "28                           3.40    915.0  \n",
      "29                           3.59   1035.0  \n",
      "..                            ...      ...  \n",
      "148                          1.62    650.0  \n",
      "149                          1.33    550.0  \n",
      "150                          1.30    500.0  \n",
      "151                          1.47    480.0  \n",
      "152                          1.33    425.0  \n",
      "153                          1.51    675.0  \n",
      "154                          1.55    640.0  \n",
      "155                          1.48    725.0  \n",
      "156                          1.64    480.0  \n",
      "157                          1.73    880.0  \n",
      "158                          1.96    660.0  \n",
      "159                          1.78    620.0  \n",
      "160                          1.58    520.0  \n",
      "161                          1.82    680.0  \n",
      "162                          2.11    570.0  \n",
      "163                          1.75    675.0  \n",
      "164                          1.68    615.0  \n",
      "165                          1.75    520.0  \n",
      "166                          1.56    695.0  \n",
      "167                          1.75    685.0  \n",
      "168                          1.80    750.0  \n",
      "169                          1.92    630.0  \n",
      "170                          1.83    510.0  \n",
      "171                          1.63    470.0  \n",
      "172                          1.71    660.0  \n",
      "173                          1.74    740.0  \n",
      "174                          1.56    750.0  \n",
      "175                          1.56    835.0  \n",
      "176                          1.62    840.0  \n",
      "177                          1.60    560.0  \n",
      "\n",
      "[178 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define the data/predictors as the pre-set feature names  \n",
    "df = pd.DataFrame(winedata.data, columns=winedata.feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MEDV\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "..    ...\n",
      "148     2\n",
      "149     2\n",
      "150     2\n",
      "151     2\n",
      "152     2\n",
      "153     2\n",
      "154     2\n",
      "155     2\n",
      "156     2\n",
      "157     2\n",
      "158     2\n",
      "159     2\n",
      "160     2\n",
      "161     2\n",
      "162     2\n",
      "163     2\n",
      "164     2\n",
      "165     2\n",
      "166     2\n",
      "167     2\n",
      "168     2\n",
      "169     2\n",
      "170     2\n",
      "171     2\n",
      "172     2\n",
      "173     2\n",
      "174     2\n",
      "175     2\n",
      "176     2\n",
      "177     2\n",
      "\n",
      "[178 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Put the target (wine value -- MEDV) in another DataFrame\n",
    "target = pd.DataFrame(winedata.target, columns=['MEDV'])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use statsmodels and run a regression without constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = df['alcohol'] ## X usually means our input variables (or independent variables)\n",
    "y = target[\"MEDV\"] ## Y usually means our output/dependent variable\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      14.23\n",
      "1      13.20\n",
      "2      13.16\n",
      "3      14.37\n",
      "4      13.24\n",
      "5      14.20\n",
      "6      14.39\n",
      "7      14.06\n",
      "8      14.83\n",
      "9      13.86\n",
      "10     14.10\n",
      "11     14.12\n",
      "12     13.75\n",
      "13     14.75\n",
      "14     14.38\n",
      "15     13.63\n",
      "16     14.30\n",
      "17     13.83\n",
      "18     14.19\n",
      "19     13.64\n",
      "20     14.06\n",
      "21     12.93\n",
      "22     13.71\n",
      "23     12.85\n",
      "24     13.50\n",
      "25     13.05\n",
      "26     13.39\n",
      "27     13.30\n",
      "28     13.87\n",
      "29     14.02\n",
      "       ...  \n",
      "148    13.32\n",
      "149    13.08\n",
      "150    13.50\n",
      "151    12.79\n",
      "152    13.11\n",
      "153    13.23\n",
      "154    12.58\n",
      "155    13.17\n",
      "156    13.84\n",
      "157    12.45\n",
      "158    14.34\n",
      "159    13.48\n",
      "160    12.36\n",
      "161    13.69\n",
      "162    12.85\n",
      "163    12.96\n",
      "164    13.78\n",
      "165    13.73\n",
      "166    13.45\n",
      "167    12.82\n",
      "168    13.58\n",
      "169    13.40\n",
      "170    12.20\n",
      "171    12.77\n",
      "172    14.16\n",
      "173    13.71\n",
      "174    13.40\n",
      "175    13.27\n",
      "176    13.17\n",
      "177    14.13\n",
      "Name: alcohol, Length: 178, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Coefficient (coef):** Means that as the RM variable increases by 1, the predicted value of MDEV increases by 3.634\n",
    "\n",
    "**R-squared:** Is the percentage of *variance* our model explains (also called coefficient of determination).\n",
    "- Best possible score is 1.0 and it can be negative (because the model can be worse than random!). \n",
    "- A constant model that always predicts the expected value of y, disregarding the input features, would get a R-squared score of 0.0.\n",
    "\n",
    "**The standard error:** standard deviation of the sampling distribution of a statistic, most commonly of the mean.\n",
    "\n",
    "**The t scores and p-values:** Useful for hypothesis testing. It means that RM has statistically significant p-value; we predict at a 95% percent confidence that the value of RM is between 3.548 to 3.759."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   238.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>1.36e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:37:12</td>     <th>  Log-Likelihood:    </th> <td> -211.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   178</td>      <th>  AIC:               </th> <td>   424.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   177</td>      <th>  BIC:               </th> <td>   428.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alcohol</th> <td>    0.0707</td> <td>    0.005</td> <td>   15.431</td> <td> 0.000</td> <td>    0.062</td> <td>    0.080</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.051</td> <th>  Durbin-Watson:     </th> <td>   0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  13.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.017</td>  <th>  Prob(JB):          </th> <td> 0.00113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.647</td>  <th>  Cond. No.          </th> <td>    1.00</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.574\n",
       "Model:                            OLS   Adj. R-squared:                  0.571\n",
       "Method:                 Least Squares   F-statistic:                     238.1\n",
       "Date:                Wed, 11 Jul 2018   Prob (F-statistic):           1.36e-34\n",
       "Time:                        22:37:12   Log-Likelihood:                -211.45\n",
       "No. Observations:                 178   AIC:                             424.9\n",
       "Df Residuals:                     177   BIC:                             428.1\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "alcohol        0.0707      0.005     15.431      0.000       0.062       0.080\n",
       "==============================================================================\n",
       "Omnibus:                      214.051   Durbin-Watson:                   0.022\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               13.578\n",
       "Skew:                          -0.017   Prob(JB):                      0.00113\n",
       "Kurtosis:                       1.647   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the statistics of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use statsmodels and run a regression with constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   21.25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>7.72e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:37:12</td>     <th>  Log-Likelihood:    </th> <td> -196.56</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   178</td>      <th>  AIC:               </th> <td>   397.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   176</td>      <th>  BIC:               </th> <td>   403.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    5.0119</td> <td>    0.885</td> <td>    5.660</td> <td> 0.000</td> <td>    3.264</td> <td>    6.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alcohol</th> <td>   -0.3133</td> <td>    0.068</td> <td>   -4.610</td> <td> 0.000</td> <td>   -0.447</td> <td>   -0.179</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>40.763</td> <th>  Durbin-Watson:     </th> <td>   0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  20.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.659</td> <th>  Prob(JB):          </th> <td>4.04e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.004</td> <th>  Cond. No.          </th> <td>    211.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.108\n",
       "Model:                            OLS   Adj. R-squared:                  0.103\n",
       "Method:                 Least Squares   F-statistic:                     21.25\n",
       "Date:                Wed, 11 Jul 2018   Prob (F-statistic):           7.72e-06\n",
       "Time:                        22:37:12   Log-Likelihood:                -196.56\n",
       "No. Observations:                 178   AIC:                             397.1\n",
       "Df Residuals:                     176   BIC:                             403.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          5.0119      0.885      5.660      0.000       3.264       6.759\n",
       "alcohol       -0.3133      0.068     -4.610      0.000      -0.447      -0.179\n",
       "==============================================================================\n",
       "Omnibus:                       40.763   Durbin-Watson:                   0.101\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               20.233\n",
       "Skew:                           0.659   Prob(JB):                     4.04e-05\n",
       "Kurtosis:                       2.004   Cond. No.                         211.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm # import statsmodels \n",
    "\n",
    "X = df['alcohol'] \n",
    "y = target[\"MEDV\"] \n",
    "X = sm.add_constant(X) ## Add an intercept or constant to the model\n",
    "\n",
    "model = sm.OLS(y, X).fit() ## sm.OLS(output, input)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['alcohol','malic_acid']]\n",
    "y = target['MEDV'] \n",
    "X = sm.add_constant(X) ## Add an intercept or constant to the model\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- This model has a much higher R-squared value *0.948*, meaning that this model explains 94.8% of the variance in our dependent variable. (Most of the time, when we add variables to a regression model, R will be higher..)\n",
    "- RM and LSTAT are statistically significant in predicting (or estimating) the median house value \n",
    "- As RM increases by 1, MEDV will increase by 5.0948 and when LSTAT increases by 1, MEDV will decrease by -0.6424. \n",
    "- LSTAT: is the percentage of lower status of the populatio\n",
    "- RM: more rooms in a house (usually the higher, the value of the house will become higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>6.51e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:37:12</td>     <th>  Log-Likelihood:    </th> <td> -171.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   178</td>      <th>  AIC:               </th> <td>   348.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   175</td>      <th>  BIC:               </th> <td>   357.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>    4.7994</td> <td>    0.770</td> <td>    6.231</td> <td> 0.000</td> <td>    3.279</td> <td>    6.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alcohol</th>    <td>   -0.3560</td> <td>    0.059</td> <td>   -5.997</td> <td> 0.000</td> <td>   -0.473</td> <td>   -0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>malic_acid</th> <td>    0.3281</td> <td>    0.043</td> <td>    7.607</td> <td> 0.000</td> <td>    0.243</td> <td>    0.413</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.127</td> <th>  Durbin-Watson:     </th> <td>   0.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.077</td> <th>  Jarque-Bera (JB):  </th> <td>   4.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.341</td> <th>  Prob(JB):          </th> <td>  0.0913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.574</td> <th>  Cond. No.          </th> <td>    214.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.329\n",
       "Model:                            OLS   Adj. R-squared:                  0.322\n",
       "Method:                 Least Squares   F-statistic:                     42.99\n",
       "Date:                Wed, 11 Jul 2018   Prob (F-statistic):           6.51e-16\n",
       "Time:                        22:37:12   Log-Likelihood:                -171.14\n",
       "No. Observations:                 178   AIC:                             348.3\n",
       "Df Residuals:                     175   BIC:                             357.8\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.7994      0.770      6.231      0.000       3.279       6.320\n",
       "alcohol       -0.3560      0.059     -5.997      0.000      -0.473      -0.239\n",
       "malic_acid     0.3281      0.043      7.607      0.000       0.243       0.413\n",
       "==============================================================================\n",
       "Omnibus:                        5.127   Durbin-Watson:                   0.573\n",
       "Prob(Omnibus):                  0.077   Jarque-Bera (JB):                4.787\n",
       "Skew:                           0.341   Prob(JB):                       0.0913\n",
       "Kurtosis:                       2.574   Cond. No.                         214.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Linear Regression with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data/predictors as the pre-set feature names  \n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Put the target (housing value -- MEDV) in another DataFrame\n",
    "target = pd.DataFrame(data.target, columns=[\"MEDV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['alcohol','malic_acid']]\n",
    "y = target['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.295053   0.68467237 0.88922774 0.32396874 0.93622041]\n"
     ]
    }
   ],
   "source": [
    "predictions = lm.predict(X)\n",
    "print(predictions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3294405905959348"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is thre R-squared of the model\n",
    "lm.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7994146319956235"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the intercept (constant)\n",
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.355971  ,  0.32813196])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the coefficients\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
